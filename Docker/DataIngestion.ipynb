{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import boto3\n",
    "from boto3.s3.transfer import S3Transfer\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def timeSeriesDataIngestion():\n",
    "    \n",
    "    homepath = os.path.expanduser('~')\n",
    "    \n",
    "    country_list = ['BRA', 'IND', 'ZAF', 'ECU', 'ARG', 'LBY']\n",
    "\n",
    "    indicator_list = ['AG.LND.AGRI.ZS', 'SP.POP.DPND', 'SP.DYN.CBRT.IN', 'NE.EXP.GNFS.ZS', 'NY.GDP.MKTP.CD', \\\n",
    "                      'NY.GDP.MKTP.KD.ZG', 'SP.POP.GROW', 'FI.RES.TOTL.CD', 'NE.TRD.GNFS.ZS']\n",
    "    \n",
    "    indicator_value = []\n",
    "\n",
    "    for country in country_list:\n",
    "        for indicator in indicator_list:\n",
    "            indicator_response = requests.get('http://api.worldbank.org/countries/' \\\n",
    "                                              + country + '/indicators/' + indicator + '?format=json&per_page=1000')\n",
    "\n",
    "            filepath = './Data/TimeSeries/' + indicator + '/'\n",
    "            filename = filepath + country + '.json'\n",
    "        \n",
    "            if not os.path.exists(filepath):            \n",
    "                print('Creating required directories for Timeseries!!', '\\n')\n",
    "                os.makedirs(filepath)\n",
    "            \n",
    "            with open(filename, 'w') as f:\n",
    "                f.write(indicator_response.text)\n",
    "\n",
    "            indicator_response_toFile = open(filename, 'r', errors = 'ignore') # Opening the file\n",
    "            indicator_data = json.load(indicator_response_toFile)            \n",
    "\n",
    "            if indicator_data[0]['total'] > 0:\n",
    "                for data in indicator_data[1:]:\n",
    "                    for yeardata in data:                \n",
    "                        indicator_value.append([yeardata['country']['value'], yeardata['country']['id'], \\\n",
    "                                                yeardata['indicator']['id'], yeardata['value'], \\\n",
    "                                                yeardata['indicator']['value'], yeardata['date'] + '-01-01'])\n",
    "    \n",
    "    headers = ['CountryName', 'CountryCode', 'IndicatorCode', 'Value', 'Value Description', 'Year']\n",
    "    indicator_df = pd.DataFrame(indicator_value, columns=headers)\n",
    "    \n",
    "    actual_filename = './Data/TimeSeries/Indicators_TimeSeries_Combined.csv'\n",
    "    indicator_df.to_csv(actual_filename, index=False)\n",
    "    print('Timeseries file created!!', '\\n')\n",
    "\n",
    "    #indicator_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clusteringDataIngestion():\n",
    "    \n",
    "    homepath = os.path.expanduser('~')\n",
    "    \n",
    "    country_list = ['AUS', 'CAN', 'SAU', 'USA', \\\n",
    "                    'IND', 'RUS', 'ZAF', 'TUR', \\\n",
    "                    'ARG', 'BRA', 'MEX', \\\n",
    "                    'FRA', 'DEU', 'ITA', 'GBR', \\\n",
    "                    'CHN', 'IDN', 'JPN']\n",
    "\n",
    "    indicator_list = ['AG.LND.AGRI.ZS', 'SP.POP.DPND', 'SP.DYN.CBRT.IN', 'NE.EXP.GNFS.ZS', 'NE.IMP.GNFS.ZS', \\\n",
    "                      'NY.GDP.MKTP.CD', 'NY.GDP.MKTP.KD.ZG', 'SP.POP.GROW', 'FI.RES.TOTL.CD', 'NE.TRD.GNFS.ZS']\n",
    "    \n",
    "    indicator_value = []\n",
    "\n",
    "    for country in country_list:\n",
    "        for indicator in indicator_list:\n",
    "            indicator_response = requests.get('http://api.worldbank.org/countries/' \\\n",
    "                                              + country + '/indicators/' + indicator + \\\n",
    "                                              '?format=json&per_page=1000&date=2001:2016')\n",
    "\n",
    "            filepath = './Data/Clustering/' + indicator + '/'\n",
    "            filename = filepath + country + '.json'\n",
    "        \n",
    "            if not os.path.exists(filepath):            \n",
    "                print('Creating required directories for Clustering!!', '\\n')\n",
    "                os.makedirs(filepath)\n",
    "            \n",
    "            with open(filename, 'w') as f:\n",
    "                f.write(indicator_response.text)\n",
    "\n",
    "            indicator_response_toFile = open(filename, 'r', errors = 'ignore') # Opening the file\n",
    "            indicator_data = json.load(indicator_response_toFile)            \n",
    "\n",
    "            if indicator_data[0]['total'] > 0:\n",
    "                for data in indicator_data[1:]:\n",
    "                    for yeardata in data:                \n",
    "                        indicator_value.append([yeardata['country']['value'], yeardata['country']['id'], \\\n",
    "                                                yeardata['indicator']['id'], yeardata['value'], \\\n",
    "                                                yeardata['indicator']['value'], yeardata['date'] + '-01-01'])\n",
    "    \n",
    "    headers = ['CountryName', 'CountryCode', 'IndicatorCode', 'Value', 'Value Description', 'Year']\n",
    "    indicator_df = pd.DataFrame(indicator_value, columns=headers)\n",
    "    \n",
    "    actual_filename = './Data/Clustering/Indicators_Clustering_Combined.csv'\n",
    "    indicator_df.to_csv(actual_filename, index=False)\n",
    "    print('Clustering file created!!', '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timeseriesFileUploadToS3(AWS_ACCESS_KEY, AWS_SECRET_KEY):\n",
    "    \n",
    "    conn = boto3.client('s3', aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY)\n",
    "    transfer = S3Transfer(conn)\n",
    "\n",
    "    response = conn.list_buckets()    \n",
    "    existent = []\n",
    "    for bucket in response[\"Buckets\"]:\n",
    "        existent.append(bucket['Name'])\n",
    "\n",
    "    bucket_name = 'Team6FinalProject'\n",
    "    target_dir = './Data/TimeSeries/'\n",
    "    filenames = []\n",
    "    file_list = os.listdir(target_dir)\n",
    "    for file in file_list:\n",
    "        if '_Combined' in file:\n",
    "            filenames.append(file)\n",
    "\n",
    "    if bucket_name in existent:\n",
    "        print('Bucket already exists!!', '\\n')\n",
    "        print('Timeseries Files upload started to s3!!!!!', '\\n')\n",
    "        for files in filenames:\n",
    "            upload_filename = 'TimeSeries/'+files\n",
    "            transfer.upload_file(os.path.join(target_dir, files), bucket_name, upload_filename)\n",
    "        print('Timeseries Files uploaded to s3!!!!!','\\n')\n",
    "            \n",
    "    else:\n",
    "        print('Bucket not present. Created bucket!!', '\\n')\n",
    "        conn.create_bucket(Bucket=bucket_name, ACL='public-read-write')\n",
    "        print('Timeseries Files upload started to s3!!!!!', '\\n')\n",
    "        for files in filenames:\n",
    "            upload_filename = 'TimeSeries/'+files\n",
    "            transfer.upload_file(os.path.join(target_dir, files), bucket_name, upload_filename)\n",
    "        print('Timeseries Files uploaded to s3!!!!!','\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clusteringFileUploadToS3(AWS_ACCESS_KEY, AWS_SECRET_KEY):\n",
    "    \n",
    "    conn = boto3.client('s3', aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY)\n",
    "    transfer = S3Transfer(conn)\n",
    "\n",
    "    response = conn.list_buckets()    \n",
    "    existent = []\n",
    "    for bucket in response[\"Buckets\"]:\n",
    "        existent.append(bucket['Name'])\n",
    "\n",
    "    bucket_name = 'Team6FinalProject'\n",
    "    homepath = os.path.expanduser('~')\n",
    "    target_dir = './Data/Clustering/'\n",
    "    filenames = []\n",
    "    file_list = os.listdir(target_dir)\n",
    "    for file in file_list:\n",
    "        if '_Combined' in file:\n",
    "            filenames.append(file)\n",
    "\n",
    "    if bucket_name in existent:\n",
    "        print('Bucket already exists!!', '\\n')\n",
    "        print('CLustering Files upload started to s3!!!!!', '\\n')\n",
    "        for files in filenames:\n",
    "            upload_filename = 'Clustering/'+files\n",
    "            transfer.upload_file(os.path.join(target_dir, files), bucket_name, upload_filename, \\\n",
    "                                 extra_args={'ACL': 'public-read'})\n",
    "        print('Clustering Files uploaded to s3!!!!!','\\n')\n",
    "            \n",
    "    else:\n",
    "        print('Bucket not present. Created bucket!!', '\\n')\n",
    "        conn.create_bucket(Bucket=bucket_name, ACL='public-read-write')\n",
    "        print('CLustering Files upload started to s3!!!!!', '\\n')\n",
    "        for files in filenames:\n",
    "            upload_filename = 'Clustering/'+files\n",
    "            transfer.upload_file(os.path.join(target_dir, files), bucket_name, upload_filename, \\\n",
    "                                 extra_args={'ACL': 'public-read'})\n",
    "        print('CLustering Files uploaded to s3!!!!!','\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    user_input = sys.argv[1:]\n",
    "    print(\"----Process Started----\")\n",
    "    counter = 0\n",
    "    if len(user_input) == 0:\n",
    "        print('No Input provided. Process is exiting!!')\n",
    "        exit(0)\n",
    "    for ip in user_input:\n",
    "        if counter == 0:\n",
    "            AWS_ACCESS_KEY = str(ip)\n",
    "        else:\n",
    "            AWS_SECRET_KEY = str(ip)\n",
    "        counter += 1\n",
    "    \n",
    "    timeSeriesDataIngestion()\n",
    "    clusteringDataIngestion()\n",
    "    timeseriesFileUploadToS3(AWS_ACCESS_KEY, AWS_SECRET_KEY)\n",
    "    clusteringFileUploadToS3(AWS_ACCESS_KEY, AWS_SECRET_KEY)\n",
    "    \n",
    "    print('Process completed!!','\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
